---
title: "Integrating STT and TTS with Deepgram for Low Latency"
description: "Guide to integrating Deepgram's speech-to-text and text-to-speech APIs for low-latency applications."
summary: "This article explores architectural considerations for seamlessly integrating Deepgram's speech-to-text and text-to-speech capabilities with low-latency applications, including maintaining open connections, handling transitions between technologies, and managing large language models."
---

<CommunityQuestion>I'm building a voice assistant that needs to respond in real-time with low latency. How can I integrate Deepgram's speech-to-text and text-to-speech APIs into my application to achieve this seamlessly?</CommunityQuestion>

## Integrating STT and TTS for LLM Applications

When integrating Speech-to-Text (STT) and Text-to-Speech (TTS) capabilities using Deepgram with applications that also include language learning models (LLMs), there are several architectural considerations to ensure a smooth and low-latency experience.

### Maintaining Connections for Low Latency

For STT functionality, maintaining an open connection is generally recommended. Deepgram’s live transcription WebSocket API is designed to efficiently handle continuous streams of data. By keeping the connection open, interim results can be returned, providing a real-time transcription experience that balances speed and accuracy. Pooling connections and reusing them as necessary can mitigate the concerns around having too many open connections or delays in establishing new ones.

### Transition Between TTS and STT

An important aspect of the architecture is handling the transition between TTS output and restarting STT input. This can be achieved by implementing state management that detects when the TTS playback concludes. Using an audio API, TTS playback can be monitored, and upon completion, a state change can trigger the resumption of STT.

To improve the flow, a local voice activity detection (VAD) system can be used, allowing the user to interrupt the TTS playback. By detecting speech, the system can pause or cancel the ongoing TTS and process the new input immediately.

### Overall Architecture Considerations

For optimal performance, the architecture typically sees the client establish a WebSocket connection directly with Deepgram’s API, managing the sessions for both STT and TTS. Though anyone considering this must weigh the security against the latency benefits. Keeping API keys client-side might introduce vulnerabilities unless mitigated by the use of short-lived tokens or other security measures.

For deploying the backend, consider the complexity of managing API key lifecycle and access control to ensure security while maintaining a seamless experience.

## Conclusion

Building a responsive system with STT and TTS requires careful consideration of connection management and state transitions between speech recognition and generation processes. Using Deepgram's real-time WebSocket APIs, you can maintain open connections for better responsiveness and integrate VAD to smooth out the interaction flow between your voice interface components.

### References
- [Deepgram Live Transcription Docs](https://developers.deepgram.com/docs/getting-started-with-live-streaming-audio)
- [Deepgram Text-to-Speech Docs](https://developers.deepgram.com/docs/tts-websocket)