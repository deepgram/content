---
title: "Optimizing End of Speech Detection in Noisy Environments"
description: "Strategies for improving Deepgram's end of speech detection in noisy environments."
summary: "This article provides guidance on configuring Deepgram's speech-to-text API to enhance end of speech detection accuracy when working with audio containing significant background noise, such as phone call recordings."
---

<CommunityQuestion>I'm working on a project that involves transcribing phone calls with a lot of background noise. The standard settings on Deepgram's API aren't handling the noisy audio very well. Could anyone offer some tips on optimising the end of speech detection for these kinds of challenging environments?</CommunityQuestion>

In projects involving speech recognition, particularly in scenarios where an AI assistant analyzes phone call audio, optimizing end of speech detection becomes pivotal. This can be challenging, especially in environments with significant background noise. This article walks you through strategies to improve end of speech detection using Deepgram's API, optimizing configurations for more reliable results.

### Configuring Deepgram API for Noisy Environments

When working with audio that may contain background noise, the default parameters might not achieve the best results for detecting the end of speech. Adjusting parameters such as `endpointing` and `utterance_end_ms` within the Deepgram API can aid in fine-tuning speech detection in these conditions:

- **`endpointing` Parameter**: This setting controls the sensitivity of end of speech detection. Lower values make the system more sensitive to detecting the end of speech by identifying shorter pauses. You may need to decrease this value to ensure that the model doesn't miss speech endings in a noisier environment.

- **`utterance_end_ms` Parameter**: This determines how long the system waits before considering speech ended. By increasing this value, the system is less likely to interpret pauses as the end of the speech, which can be beneficial in speech scenarios with intermittent noise.

```markdown
- Sample Configuration:

  ```json
  {
    "encoding": "mulaw",
    "sample_rate": "8000",
    "model": "nova-2",
    "smart_format": true,
    "interim_results": true,
    "endpointing": 150,
    "utterance_end_ms": 1200
  }
  ```

**Testing and Optimization**: Adjust and test these parameters iteratively. Start with the baseline settings and progressively tune them while testing with actual noisy audio data. It's crucial to find the right balance that captures the end of speech without excessive latency or false detections from noise.

### Selecting the Right Model

Different models may perform differently under varying conditions. Considering an environment with phone call audio, potentially using a model trained specifically for this type of data can yield improvements. Deepgram's `nova-2-phonecall` model might provide enhancements in terms of performance as it's optimized for this specific context.

You can [find more about models and their contexts here](https://developers.deepgram.com/docs/models-languages-overview#nova-2).

### Conclusion

Adjusting speech detection sensitivity in the presence of noise can greatly enhance the responsiveness and accuracy of AI assistants in phone call environments. By carefully modifying the `endpointing` and `utterance_end_ms` parameters and selecting the appropriate model for the situation, users can optimize the performance of their applications. Continuous testing with real-world data remains critical to identify the ideal settings.

If issues persist or the system behavior seems inconsistent, reach out to your Deepgram support representative (if you have one) or visit our community for assistance: [Deepgram Community](https://discord.gg/deepgram).

### References
- [Deepgram Models & Languages Overview](https://developers.deepgram.com/docs/models-languages-overview#nova-2)
- [Deepgram Pre-recorded Audio API Documentation](https://developers.deepgram.com/docs/getting-started-with-pre-recorded-audio)