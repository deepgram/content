---
title: Segmentación de texto para la optimización de TTS en streaming
subtitle: >-
  La segmentación de texto es el proceso de dividir entradas de texto en fragmentos más pequeños y manejables antes de su procesamiento.
slug: docs/segmentacion-de-texto-para-optimizacion-tts-en-streaming
---


## Ejemplos de código para WebSockets TTS

La idea detrás del uso de WebSockets con Texto a Voz (TTS) es evitar escribir audio en disco para su reproducción, y en su lugar tener un flujo continuo de datos de audio hacia el dispositivo de reproducción. Esto puede presentar algunos desafíos, especialmente respecto a la latencia del flujo de audio, el formato del audio y el propio dispositivo reproductor.

### Ejemplo sencillo usando WebSocket con audio

En la mayoría de los casos, el siguiente ejemplo ilustra un caso sencillo donde se toma un flujo de audio TTS y se envía directamente a un dispositivo reproductor como un altavoz. La mayoría de los dispositivos hardware actuales soportan audio en formato `linear16`. Esto es suficiente para su uso en ordenadores portátiles (Linux, MacOS, Windows), dispositivos IoT/Edge y muchas otras plataformas de hardware.

<CodeGroup>
  ```python Python
  import sounddevice as sd
  import numpy as np
  import time

  from deepgram import (
      DeepgramClient,
      DeepgramClientOptions,
      SpeakWebSocketEvents,
      SpeakOptions,
  )

  TTS_TEXT = "Hola, este es un ejemplo de texto a voz usando Deepgram."

  def main():
      try:
          # Crear un cliente Deepgram usando la clave API desde variables de entorno
          deepgram: DeepgramClient = DeepgramClient()

          # Crear una conexión websocket con Deepgram
          dg_connection = deepgram.speak.websocket.v("1")

          def on_open(self, open, **kwargs):
              print(f"\n\n{open}\n\n")

          def on_binary_data(self, data, **kwargs):
              print("Recibidos datos binarios")
              array = np.frombuffer(data, dtype=np.int16)
              sd.play(array, 48000)
              sd.wait()

          def on_close(self, close, **kwargs):
              print(f"\n\n{close}\n\n")

          dg_connection.on(SpeakWebSocketEvents.Open, on_open)
          dg_connection.on(SpeakWebSocketEvents.AudioData, on_binary_data)
          dg_connection.on(SpeakWebSocketEvents.Close, on_close)

          # conectar al websocket
          options = SpeakOptions(
              model="aura-asteria-en",
              encoding="linear16",
              sample_rate=48000,
          )

          print("\n\nPulsa Enter para detener...\n\n")
          if dg_connection.start(options) is False:
              print("Error al iniciar la conexión")
              return

          # enviar el texto a Deepgram
          dg_connection.send_text(TTS_TEXT)
          dg_connection.flush()

          # Indicamos que hemos terminado
          time.sleep(5)
          print("\n\nPulsa Enter para detener...\n\n")
          input()
          dg_connection.finish()

          print("Finalizado")
          
      except Exception as e:
          print(f"Ocurrió un error inesperado: {e}")

  if __name__ == "__main__":
      main()
  ```
</CodeGroup>

## Consideraciones sobre WebSockets

Al utilizar la segmentación de texto como estrategia para minimizar la latencia, se deben tener en cuenta factores como los siguientes:

* Expectativas del usuario: Considera las preferencias y necesidades de los usuarios, tales como su tolerancia a la latencia, la calidad de voz sintetizada deseada y su satisfacción general con el rendimiento de la aplicación.
* De manera predeterminada, solo se admitirá que el WebSocket envíe audio parcialmente segmentado.

### Para WebSockets

* Optimiza la latencia: Aunque gestionamos toda la segmentación por ti para optimizar el equilibrio entre latencia y naturalidad del habla, intenta experimentar con diferentes longitudes de segmentos de frase (por ejemplo segmentando antes de la primera “,” o “.”) en la primera frase de tu respuesta para optimizar aún más la latencia del tiempo del primer byte obtenido desde una salida generada por un modelo de lenguaje grande (LLM).